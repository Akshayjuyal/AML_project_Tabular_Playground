{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgwZ0bHeLBTdQxj8DVbfku"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uRNfb2xbWWd","executionInfo":{"status":"ok","timestamp":1667192720297,"user_tz":240,"elapsed":22709,"user":{"displayName":"Joanne Wardell","userId":"01069754557761907673"}},"outputId":"2b591b29-0b6f-4434-eacd-bd3eaa732122"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":73,"metadata":{"id":"50x_ZP536NYX","executionInfo":{"status":"ok","timestamp":1667192198266,"user_tz":240,"elapsed":32527,"user":{"displayName":"Joanne Wardell","userId":"01069754557761907673"}}},"outputs":[],"source":["import math\n","import numpy as np\n","import pandas as pd\n","import sys\n","'''\n","new approach: \n","in the train step, compute class conditional densities\n","use kernels to estimate probability of x\n","'''\n","\n","class NaiveBayesClassifier:\n","  \n","  def __init__(self, kernel_type, h):\n","    if(kernel_type == 'hypercube'):\n","      self.kernel = self.hypercube\n","    if(kernel_type == 'rbf'):\n","      self.kernel = self.rbf\n","    self.h = h\n","    \n","\n","  def hypercube(self, x):\n","    for i in range(len(x)):\n","      if(x[i] >= .5):\n","        return 0\n","    return 1\n","\n","  def rbf(self, x):\n","    #xTx = np.dot(x,x)\n","    xTx = 0\n","    for i in range(len(x)):\n","      if(math.isnan(x[i])):\n","        continue\n","      xTx += float(x[i])**2\n","    xTx = math.sqrt(xTx)\n","    result = math.pow(2*math.pi,(-self.dim/2)) * math.pow(math.e,-(1/2)*xTx)\n","    #print(\"rbf returning {}\".format(result))\n","    return result\n","\n","  def pdke(self, x):\n","    result = 0\n","    for i in range(self.N):\n","      result += self.kernel((x-self.test_data[i])/self.h)\n","    return result/(self.N*self.h**self.dim)\n","\n","  def fit(self, training_file):\n","    train_df = pd.read_csv(training_file)\n","    self.dim = train_df.shape[1]\n","    self.N = train_df.shape[0]\n","\n","    self.priors = dict()\n","\n","    self.priors['A'] =  (np.array(train_df['team_scoring_next'])=='A').sum() / self.N\n","    self.priors['B'] =  (np.array(train_df['team_scoring_next'])=='B').sum() / self.N\n","\n","    if(self.priors['A'] == 0):\n","      self.priors['A'] = 1 - self.priors['B']\n","\n","    if(self.priors['B'] == 0):\n","      self.priors['B'] = 1 - self.priors['A']\n","    \n","\n","\n","  def predict(self, testing_file):\n","    test_df = pd.read_csv(testing_file)\n","    self.test_data = np.array(test_df)\n","\n","    self.N = test_df.shape[0]\n","\n","    \n","\n","    submission_df = pd.DataFrame(columns=[\"id\",\"team_A_scoring_within_10sec\",\"team_B_scoring_within_10sec\"])\n","\n","    final_labels = []\n","\n","    for i in range(len(test_df)):\n","      \n","      pa_x = 0\n","      pb_x = 0\n","\n","\n","      pkde = self.pdke(self.test_data[i])\n","\n","      pa_x = pkde * self.priors['A']\n","\n","      pb_x = pkde * self.priors['B']\n","\n","      final_labels.insert(i,[i,pa_x,pb_x])\n","      \n","\n","    final_labels = self.normalize(final_labels)\n","\n","    for i in range(len(test_df)):\n","      submission_df = submission_df.append({'id': int(final_labels[i][0]),\n","                                            'team_A_scoring_within_10sec' : final_labels[i][1],\n","                                            'team_B_scoring_within_10sec' : final_labels[i][2]}, ignore_index = True)\n","\n","      submission_df['id'] = submission_df['id'].astype(int)\n","    submission_df.to_csv('out.csv', index=False)\n","\n","  def normalize(self, arr):\n","    result = []\n","    curr_min = sys.maxsize\n","    curr_max = -sys.maxsize\n","    for i in range(len(arr)):\n","      if(min(arr[i][1],arr[i][2]) < curr_min):\n","        curr_min = min(arr[i][1],arr[i][2])\n","      if(max(arr[i][1],arr[i][2]) > curr_max):\n","        curr_max = max(arr[i][1],arr[i][2])\n","\n","    new_min = 0\n","    new_max = 1\n","\n","\n","    for i in range(len(arr)):\n","      a = ((arr[i][1]-curr_min)/(curr_max-curr_min))*(new_max-new_min)+new_min\n","      b = abs(1-a)\n","      result.insert(i,(arr[i][0],a,b))\n","\n","    return result\n","\n","nb1 = NaiveBayesClassifier('rbf', .001)\n","nb1.fit('train_0_truncated_tail.csv')\n","nb1.predict('test_truncated.csv')\n"]},{"cell_type":"code","source":["nb1.priors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKzQ3BYG6zqz","executionInfo":{"status":"ok","timestamp":1667190767368,"user_tz":240,"elapsed":204,"user":{"displayName":"Joanne Wardell","userId":"01069754557761907673"}},"outputId":"b2ca5789-2ec9-4263-c3e4-2c5dd347244d"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'A': 0.32199999999999995, 'B': 0.678}"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":[],"metadata":{"id":"mmbe9FVC8fp5"},"execution_count":null,"outputs":[]}]}