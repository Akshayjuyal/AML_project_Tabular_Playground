{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMitP9cNTbzz9X28T1agbNf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ulpxMbhQgld1"},"outputs":[],"source":["'''\n","CODE IMPORTS\n","'''\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import math\n","import random"]},{"cell_type":"code","source":["DEBUG_ON=False\n","'''\n","NOTES:\n","  1. ASSUME HIDDEN LAYERS ALL HAVE SAME NUMBER OF NEURONS\n","  2. ASSUME INPUT LAYER SIZE == DEGREE OF DATA OBJECT\n","  3. LABEL IS NOT TAKEN AS A FEATURE\n","  4. TAKE BETA HYPER PARAM AS 1\n","  5. ASSUME HIDDEN LAYERS IS >= 1\n","'''\n","\n","'''\n","CODE PERCEPTRON CLASS\n","'''\n","class Perceptron:\n","  def __init__(self):\n","    self.B = 1\n","    self.bias = None\n","    self.out_weights = None\n","    self.in_weights = None\n","    self.in_data = None\n","    self.out_data = None\n","    self.error = None\n","\n","\n","  def init_out_weights(self, degree):\n","    '''\n","    PARAM: DEGREE = NUM_UNITS_IN_NEXT_LAYER\n","    '''\n","    if(degree != 0):\n","      self.out_weights = np.random.rand(degree)\n","  \n","    #output layer perceptrons\n","    if(degree == 0):\n","      self.out_weights = None\n","\n","  def set_in_weights(self, in_weights):\n","    self.in_weights = in_weights\n","  \n","  def set_out_weights(self, out_weights):\n","    self.out_weights = out_weights\n","\n","  def set_in_data(self, data):\n","    self.in_data = data\n","\n","  def set_out_data(self, data):\n","    self.out_data = data\n","\n","  def set_bias(self, bias):\n","    self.bias = bias\n","\n","  def set_error(self, error):\n","    self.error = error\n","\n","  def sigmoid_activation(self, sum):\n","    Ij = sum + self.bias\n","    return 1 / (1 + math.e**(-self.B*Ij))\n","\n","\n","'''\n","CODE NEURAL NETWORK CLASS\n","'''\n","class NeuralNetwork:\n","  def __init__(self, labels):\n","    '''\n","    PROVIDE CLASS LABELS FOR TRAINING\n","    '''\n","    self.labels = set(labels)\n","    self.learning_rate = 0.8\n","\n","\n","    print(\"Creating new NN.\")\n","    \n","    print(\"Enter input layer size. This should equal number of features.\")\n","    self.input_layer_size = int(input())\n","\n","    print(\"Enter num hidden layers.\")\n","    self.num_hidden_layers = int(input())\n","\n","    print(\"Enter hidden layer size.\")\n","    self.hidden_layer_size = int(input())\n","\n","    print(\"Enter output layer size. This should equal number of classes.\")\n","    self.output_layer_size = int(input())\n","\n","    print(\"Enter num iterations.\")\n","    self.num_iterations = int(input())\n","    \n","\n","  def place_empty_perceptrons(self):\n","    '''\n","    CREATE SPACE FOR LAYERS\n","    '''\n","    self.input_layer = []\n","    self.hidden_layers = []\n","    for i in range(self.num_hidden_layers):\n","      self.hidden_layers.insert(i,[])\n","    self.output_layer = []\n","\n","\n","    '''\n","    PUT PERCEPTRONS IN INPUT LAYER\n","    '''\n","    for i in range(self.input_layer_size):\n","      self.input_layer.insert(i, Perceptron())\n","\n","    '''\n","    PUT PERCEPTRONS IN HIDDEN LAYER(S)\n","    '''\n","    for i in range(self.num_hidden_layers):\n","      for j in range(self.hidden_layer_size):\n","        self.hidden_layers[i].insert(j, Perceptron())\n","    \n","    '''\n","    PUT PERCEPTRONS IN OUTPUT LAYER\n","    '''\n","    for i in range(self.output_layer_size):\n","      self.output_layer.insert(i, Perceptron())\n","    if(DEBUG_ON):print(\"function place_empty_perceptrons complete\")\n","    if(DEBUG_ON):self.to_string()\n","\n","  def init_input_layer_weights_biases_and_data(self, data_object):\n","    '''\n","    PARAM: DATA_OBJECT = one N dimensional vector containing features of a data object\n","    The input layer should have N perceptrons. \n","    That way, each perceptron is assigned ONE feature from a data object\n","    '''\n","    for i in range(self.input_layer_size):\n","      self.input_layer[i].set_in_data(data_object[i])\n","      self.input_layer[i].set_in_weights(None)\n","      self.input_layer[i].init_out_weights(degree=self.hidden_layer_size)\n","      self.input_layer[i].set_out_data(data_object[i])\n","      self.input_layer[i].set_bias(random.uniform(0, 1))\n","      if(DEBUG_ON):print(\"function init_input_layer_weights_biases_and_data complete\")\n","      if(DEBUG_ON):self.to_string()\n","\n","  def init_hidden_layer_weights_and_biases(self):\n","    '''\n","    here we will only initialize the weights and bias for the HIDDEN layer perceptrons\n","    '''\n","    if(self.num_hidden_layers == 1):\n","      for i in range(self.hidden_layer_size):\n","        self.hidden_layers[0][i].init_out_weights(degree=self.output_layer_size)\n","        self.hidden_layers[0][i].set_bias(random.uniform(0, 1))\n","    \n","    if(self.num_hidden_layers > 1):\n","      next_layer = self.hidden_layers[1]\n","      for i in range(self.num_hidden_layers):\n","        for j in range(self.hidden_layer_size):\n","          self.hidden_layers[i][j].init_out_weights(degree=len(next_layer))\n","          self.hidden_layers[i][j].set_bias(random.uniform(0, 1))\n","        if((i == self.hidden_layers - 1) or (self.num_hidden_layers == 2)):\n","          next_layer = self.output_layer\n","          continue\n","        if(self.hidden_layers > 2):\n","          next_layer = self.hidden_layers[i+1]\n","\n","    if(self.num_hidden_layers == 1):\n","      for i in range(self.hidden_layer_size):\n","        in_weights = []\n","        for j in range(self.input_layer_size):\n","          in_weights.insert(j,self.input_layer[j].out_weights[i])\n","        self.hidden_layers[0][i].set_in_weights(in_weights)\n","\n","    if(self.num_hidden_layers > 1):\n","      prev_layer = self.input_layer\n","      for i in range(self.num_hidden_layers):#for every hidden layer\n","        in_weights = []\n","        for j in range(self.hidden_layer_size):#for every neuron in the current hidden layer\n","          for k in range(len(prev_layer)):#apply out weights of prev layer as in weights for current layer\n","            in_weights.insert(k, prev_layer[k].out_weights[j])\n","        self.hidden_layers[i].set_in_weights(in_weights)\n","        prev_layer = self.hidden_layers[i]\n","    if(DEBUG_ON):print(\"function init_hidden_layer_weights_and_biases complete\")\n","    if(DEBUG_ON):self.to_string()\n","    \n","\n","  def init_output_layer_weights_and_biases(self):\n","    for i in range(self.output_layer_size):#for each node in the output layer\n","      in_weights = []\n","      for j in range(self.hidden_layer_size):#for each node in the hidden layer\n","        in_weights.insert(j,self.hidden_layers[self.num_hidden_layers-1][j].out_weights[i])\n","      self.output_layer[i].set_in_weights(in_weights)\n","      self.output_layer[i].set_out_weights(None)\n","      self.output_layer[i].set_bias(random.uniform(0, 1))\n","    if(DEBUG_ON):print(\"function init_output_layer_weights_and_biases complete\")\n","    if(DEBUG_ON):self.to_string()\n","\n","  def compute_weighted_sum(self, node, node_ix, prev_layer):\n","    sum = 0\n","    for i in range(len(prev_layer)):\n","        sum += prev_layer[i].out_data * prev_layer[i].out_weights[node_ix]\n","    return sum\n","\n","  def forward_propagate(self, data_object):\n","    for i in range(self.input_layer_size):\n","      self.input_layer[i].in_data = data_object[i]\n","\n","    '''\n","    PROPAGATE DATA FROM INPUT LAYER INTO 1ST HIDDEN LAYER\n","    '''\n","    if(DEBUG_ON):print(\"BEGIN INPUT LAYER PROPAGATION\")\n","    for i in range(self.hidden_layer_size):\n","      curr_node = self.hidden_layers[0][i]\n","      curr_node.set_in_data(self.compute_weighted_sum(curr_node, i, self.input_layer))\n","      curr_node.set_out_data(curr_node.sigmoid_activation(curr_node.in_data))\n","    if(DEBUG_ON):print(\"INPUT LAYER PROPAGATION COMPLETE\")\n","    if(DEBUG_ON):self.to_string()\n","\n","    '''\n","    PROPAGATE DATA FROM HIDDEN LAYERS FORWARD\n","    '''\n","    if(DEBUG_ON):print(\"BEGIN HIDDEN LAYER PROPAGATION\")\n","    if(self.num_hidden_layers == 1):\n","      for i in range(self.output_layer_size):\n","        for j in range(self.hidden_layer_size):\n","          curr_node = self.output_layer[i]\n","          last_hidden_layer = self.hidden_layers[0]\n","          curr_node.set_in_data(self.compute_weighted_sum(curr_node, i, last_hidden_layer))\n","          curr_node.set_out_data(curr_node.sigmoid_activation(curr_node.in_data))\n","      \n","\n","    if(self.num_hidden_layers > 1):\n","      for i in range(self.num_hidden_layers):#for each hidden layer\n","        for j in range(self.hidden_layer_size):#for each neuron in the curr HL\n","          if(i == self.num_hidden_layers-1):\n","            break\n","          if(i == 0):\n","            curr_node = self.hidden_layers[i][j]\n","            curr_node.set_out_data(curr_node.sigmoid_activation(curr_node.in_data))\n","          if(i != 0):\n","            curr_node = self.hidden_layers[i][j]\n","            prev_layer = self.hidden_layers[i-1]\n","            curr_node.set_in_data(self.compute_weighted_sum(curr_node, j, prev_layer))\n","            curr_node.set_out_data(curr_node.sigmoid_activation(curr_node.in_data))\n","\n","\n","      '''\n","      time to propagate info from last hidden layer to output layer...\n","      ...if there is more than 1 hidden layer\n","      '''\n","      for i in range(self.output_layer.size):\n","          prev_layer = self.hidden_layers[self.num_hidden_layers-1]\n","          curr_node = self.output_layer[i]\n","          curr_node.set_in_data(curr_node, prev_layer)\n","          curr_node.set_out_data(self.sigmoid_activation(curr_node.in_data))\n","    \n","    if(DEBUG_ON):print(\"HIDDEN LAYER PROPAGATION COMPLETE\")\n","    if(DEBUG_ON):self.to_string()\n","\n","    \n","    '''\n","    COMPUTE OUTPUT LAYER PROBABILITY SCORES\n","    '''\n","    if(DEBUG_ON):print(\"BEGIN OUTPUT LAYER PROBABILITY CALCULATION\")\n","    self.class_probabilities = dict()\n","    j = 0\n","    for i in self.labels:\n","      self.class_probabilities[i] = self.output_layer[j].out_data\n","      j += 1 \n","    if(DEBUG_ON):print(\"forward prop done\")\n","    if(DEBUG_ON):self.to_string()\n","    if(DEBUG_ON):print(\"\\n\\tclass probabilities are {} \".format(self.class_probabilities))\n","    return self.class_probabilities\n","\n","\n","\n","  def backward_propagate(self):\n","    '''\n","    BACK PROPAGATE THE ERRORS FROM OUTPUT LAYER\n","    '''\n","    for i in range(self.output_layer_size):\n","      curr_node = self.output_layer[i]\n","      curr_out_data = curr_node.out_data\n","      curr_node.set_error(curr_out_data * (1-curr_out_data) * (int(np.array(list(self.labels))[i]) - curr_out_data))\n","\n","    '''\n","    BACK PROPAGATE THE ERRORS IN HIDDEN LAYER\n","    '''\n","    if(self.num_hidden_layers == 1):\n","      for i in range(self.hidden_layer_size):\n","        curr_node = self.hidden_layers[0][i]\n","        self.find_cumulative_error(curr_node, i, self.output_layer)\n","\n","    if(self.num_hidden_layers > 1):\n","      out_layer = self.output_layer\n","      for i in range(self.num_hidden_layers, 0, -1):#for each hidden layer\n","        for j in range(self.hidden_layer_size):#for each node in the hidden layer\n","          curr_node = self.hidden_layers[i][j]\n","          self.find_cumulative_error(curr_node, j, out_layer)\n","        out_layer = self.hidden_layers[i]\n","    \n","    if(DEBUG_ON):print(\"\\n\\n\\tBACK PROP COMPLETE\\n\")\n","    if(DEBUG_ON):self.to_string()\n","\n","    '''\n","    UPDATE WEIGHTS USING ERRORS: OUTPUT TO HIDDEN LAYER\n","    '''\n","    for i in range(self.output_layer_size):\n","      curr_node = self.output_layer[i]\n","      for j in range(len(curr_node.in_weights)):\n","        prev_weight = curr_node.in_weights[j]\n","        delta_weight = self.learning_rate * curr_node.error * curr_node.out_data\n","        curr_node.in_weights[j] = prev_weight + delta_weight\n","    \n","    if(DEBUG_ON):print(\"\\n\\n\\tUPDATE WEIGHTS USING ERRORS: OUTPUT TO HIDDEN LAYER COMPLETE\\n\")\n","    if(DEBUG_ON):self.to_string()\n","\n","\n","    '''\n","    UPDATE OUT WEIGHTS FOR LAST HIDDEN LAYER: STEAL OUTPUT LAYER NEW WEIGHTS\n","    '''\n","    for i in range(self.output_layer_size):\n","      for j in range(self.hidden_layer_size):\n","        self.hidden_layers[self.num_hidden_layers-1][j].out_weights[i] = self.output_layer[i].in_weights[j]\n","\n","    if(DEBUG_ON):print(\"\\n\\n\\tUPDATE OUT WEIGHTS FOR LAST HIDDEN LAYER: STEAL OUTPUT LAYER NEW WEIGHTS COMPLETE\\n\")\n","    if(DEBUG_ON):self.to_string()\n","\n","\n","    '''\n","    UPDATE IN WEIGHTS FOR HIDDEN LAYER\n","    '''\n","    if(self.num_hidden_layers == 1):\n","      for i in range(self.hidden_layer_size):\n","        curr_node = self.hidden_layers[0][i]\n","        for j in range(len(curr_node.in_weights)):\n","          prev_weight = curr_node.in_weights[j]\n","          delta_weight = self.learning_rate * curr_node.error * curr_node.out_data\n","          curr_node.in_weights[j] = prev_weight + delta_weight\n","\n","\n","    if(self.num_hidden_layers > 1):\n","      for i in range(self.num_hidden_layers, 0, -1):\n","        curr_layer = self.hidden_layers[i]\n","        if(i == 0):\n","          prev_layer = self.input_layer\n","        if(i != 0):\n","          prev_layer = self.hidden_layers[i-1]\n","        for j in range(self.hidden_layer_size):\n","          curr_node = curr_layer[j]\n","          for k in range(len(curr_node.in_weights)):\n","            prev_weight = curr_node.in_weights[k]\n","            delta_weight = self.learning_rate * curr_node.error * curr_node.out_data\n","            curr_node.in_weight = prev_weight + delta_weight\n","    \n","    if(DEBUG_ON):print(\"\\n\\n\\tUPDATE IN WEIGHTS FOR HIDDEN LAYER COMPLETE\\n\")\n","    if(DEBUG_ON):self.to_string()\n","\n","\n","    '''\n","    UPDATE BIASES FOR EACH NEURON IN OUTPUT LAYER\n","    '''\n","    for i in range(self.output_layer_size):\n","      curr_node = self.output_layer[i]\n","      prev_bias = curr_node.bias\n","      delta_bias = self.learning_rate * curr_node.error\n","      curr_node.bias = prev_bias + delta_bias\n","    \n","    '''\n","    UPDATE BIASES FOR EACH NEURON IN HIDDEN LAYER(S)\n","    '''\n","    for i in range(self.num_hidden_layers):\n","      for j in range(self.hidden_layer_size):\n","        curr_node = self.hidden_layers[i][j]\n","        prev_bias = curr_node.bias\n","        delta_bias = self.learning_rate * curr_node.error\n","        curr_node.bias = prev_bias + delta_bias\n","      \n","    if(DEBUG_ON):print(\"\\n\\n\\tback prop done\")\n","    if(DEBUG_ON):self.to_string()\n","\n","\n","\n","\n","  def find_cumulative_error(self, curr_node, cn_ix, out_layer):\n","    cn_error = curr_node.out_data * (1-curr_node.out_data)\n","    sum = 0\n","    for i in range(len(curr_node.out_weights)):\n","      sum += curr_node.out_weights[i]* out_layer[i].out_data\n","    curr_node.set_error(cn_error * sum)\n","    return cn_error * sum\n","\n","\n","  def fit(self, x_train):\n","    self.place_empty_perceptrons()\n","    self.init_input_layer_weights_biases_and_data(x_train[0])\n","    self.init_hidden_layer_weights_and_biases()\n","    self.init_output_layer_weights_and_biases()\n","    for i in range(len(x_train)):\n","      for j in range(self.num_iterations):\n","        self.forward_propagate(x_train[i])\n","        self.backward_propagate()\n","\n","  def predict(self, x_test):\n","    y_pred = np.zeros(len(x_test))\n","    for i in range(len(x_test)):\n","      y_pred[i] = np.argmax(np.array(list(self.forward_propagate(x_test[i]))))\n","    return y_pred\n","  \n","  def accuracy(self, y_pred, y_test):\n","    num_correct = 0\n","    for i in range(len(y_pred)):\n","      if(y_pred[i] == y_test[i]):\n","        num_correct += 1\n","    return num_correct / len(y_pred)\n","\n","  def to_string(self):\n","    print(\"\\n\\nNN\\nInput Layer:\\n\")\n","    for i in range(self.input_layer_size):\n","      print(\"Node: {}\\t In Weights: {}\\t In Data: {}\\t Out Data: {}\\t Out Weights: {}\\t Bias: {}\\t Error: {}\".format(i,self.input_layer[i].in_weights, self.input_layer[i].in_data, self.input_layer[i].out_data, self.input_layer[i].out_weights, self.input_layer[i].bias, self.input_layer[i].error))\n","    \n","    print(\"\\n\\nHidden Layer(s)\\n\")\n","    for i in range(self.num_hidden_layers):\n","      print(\"Hidden Layer {}\".format(i))\n","      for j in range(self.hidden_layer_size):\n","        print(\"Node: {}\\t In Weights: {}\\t In Data: {}\\t Out Data: {}\\t Out Weights: {}\\t Bias: {}\\t Error: {}\".format(j,self.hidden_layers[i][j].in_weights, self.hidden_layers[i][j].in_data, self.hidden_layers[i][j].out_data, self.hidden_layers[i][j].out_weights, self.hidden_layers[i][j].bias, self.hidden_layers[i][j].error))\n","    \n","    print(\"\\n\\nOutput Layer\\n\")\n","    for i in range(self.output_layer_size):\n","      print(\"Node: {}\\t In Weights: {}\\t In Data: {}\\t Out Data: {}\\t Out Weights: {}\\t Bias: {}\\t Error: {}\".format(i,self.output_layer[i].in_weights, self.output_layer[i].in_data, self.output_layer[i].out_data, self.output_layer[i].out_weights, self.output_layer[i].bias, self.output_layer[i].error))"],"metadata":{"id":"hhM6BGrdhBSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","BRING IN DATA\n","'''\n","train_dataset_1 = pd.read_csv('dataset1_training.txt', delim_whitespace=True)\n","train_dataset_2 = pd.read_csv('dataset2_training.txt', delim_whitespace=True)\n","\n","test_dataset_1 = pd.read_csv('dataset1_testing.txt', delim_whitespace=True)\n","test_dataset_2 = pd.read_csv('dataset2_testing.txt', delim_whitespace=True)\n","\n","Y_train_1 = np.array(train_dataset_1.pop('label')).astype(int)\n","X_train_1 = np.array(train_dataset_1).astype(float)\n","\n","Y_train_2 = np.array(train_dataset_2.pop('label')).astype(int)\n","X_train_2 = np.array(train_dataset_2).astype(float)\n","\n","Y_test_1 = np.array(test_dataset_1.pop('label')).astype(int)\n","X_test_1 = np.array(test_dataset_1).astype(float)\n","\n","Y_test_2 = np.array(test_dataset_2.pop('label')).astype(int)\n","X_test_2 = np.array(test_dataset_2).astype(float)\n"],"metadata":{"id":"VyYDdoOYTbSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","CREATE NEW 2 NNs AND SHOW THEIR ACCURACY\n","'''\n","ffnn1 = NeuralNetwork(Y_train_1)\n","ffnn1.fit(X_train_1)\n","Y_pred_1 = ffnn1.predict(X_test_1)\n","print(\"ffnn1 accuracy is {}\\n\\n\".format(ffnn1.accuracy(Y_test_1, Y_pred_1)))\n","\n","ffnn2 = NeuralNetwork(Y_train_2)\n","ffnn2.fit(X_train_2)\n","Y_pred_2 = ffnn1.predict(X_test_2)\n","print(\"ffnn2 accuracy is {}\\n\\n\".format(ffnn1.accuracy(Y_test_2, Y_pred_2)))"],"metadata":{"id":"YSlPbzxfhdSk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669000056988,"user_tz":300,"elapsed":85514,"user":{"displayName":"Joanne Wardell","userId":"01069754557761907673"}},"outputId":"a5fb7a9e-08ce-46c8-8ad3-e1e85408172b"},"execution_count":383,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new NN.\n","Enter input layer size. This should equal number of features.\n","2\n","Enter num hidden layers.\n","1\n","Enter hidden layer size.\n","4\n","Enter output layer size. This should equal number of classes.\n","2\n","Enter num iterations.\n","10\n","ffnn1 accuracy is 0.4444444444444444\n","\n","\n","Creating new NN.\n","Enter input layer size. This should equal number of features.\n","2\n","Enter num hidden layers.\n","1\n","Enter hidden layer size.\n","4\n","Enter output layer size. This should equal number of classes.\n","2\n","Enter num iterations.\n","10\n","ffnn2 accuracy is 0.5\n","\n","\n"]}]},{"cell_type":"code","source":["'''\n","CREATE TWO SVMs AND SHOW THEIR ACCURACY\n","'''\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","clf1 = svm.SVC()\n","clf1.fit(X_train_1, Y_train_1)\n","y_pred_1_svm = clf1.predict(X_test_1)\n","print(\"svm for dataset 1 accuracy is: {}\".format(accuracy_score(Y_test_1, y_pred_1_svm)))\n","\n","clf2 = svm.SVC()\n","clf2.fit(X_train_2, Y_train_2)\n","y_pred_2_svm = clf2.predict(X_test_2)\n","print(\"svm for dataset 2 accuracy is: {}\".format(accuracy_score(Y_test_2, y_pred_2_svm)))"],"metadata":{"id":"0l7vOx6hHWuE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668999776765,"user_tz":300,"elapsed":171,"user":{"displayName":"Joanne Wardell","userId":"01069754557761907673"}},"outputId":"d5b91bf7-af06-47c3-8d8d-f3002b79ddb9"},"execution_count":382,"outputs":[{"output_type":"stream","name":"stdout","text":["svm for dataset 1 accuracy is: 1.0\n","svm for dataset 2 accuracy is: 1.0\n"]}]}]}